<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MMPB: It's Time for Multi-Modal Personalization.">
  <meta name="keywords" content="MMPB">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMPB: It's Time for Multi-Modal Personalization</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
<style>
  .data-table {
    width: 100%;
    border-collapse: collapse;
    margin: 25px 0;
    font-size: 0.9em;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
    border-radius: 5px;
    overflow: hidden;
  }
  
  .data-table thead tr {
    background-color: #3273dc;
    color: #ffffff;
    text-align: left;
  }
  
  .data-table th,
  .data-table td {
    padding: 12px 15px;
    border-bottom: 1px solid #dddddd;
  }
  
  .data-table tbody tr {
    border-bottom: 1px solid #dddddd;
  }
  
  .data-table tbody tr:nth-of-type(even) {
    background-color: #f3f3f3;
  }
  
  .data-table tbody tr:last-of-type {
    border-bottom: 2px solid #3273dc;
  }
  
  .data-table tbody tr.active-row {
    font-weight: bold;
    color: #3273dc;
    background-color: #f0f8ff;
  }
  
  .sort-btn {
    background: none;
    border: none;
    padding: 0;
    margin-left: 5px;
    cursor: pointer;
    color: white;
  }
  
  .expand-btn {
    background-color: #3273dc;
    color: white;
    border: none;
    border-radius: 4px;
    padding: 6px 12px;
    cursor: pointer;
    transition: background-color 0.3s;
  }
  
  .expand-btn:hover {
    background-color: #276cda;
  }
  
  .row-details {
    display: none;
    padding: 10px;
    background-color: #f9f9f9;
    border-top: 1px solid #ddd;
  }
  
  .row-details.visible {
    display: table-row;
  }
  
  .filter-controls {
    margin-bottom: 20px;
    display: flex;
    gap: 10px;
    flex-wrap: wrap;
  }
  
  .filter-btn {
    background-color: #f5f5f5;
    border: 1px solid #ddd;
    border-radius: 4px;
    padding: 8px 15px;
    cursor: pointer;
    transition: all 0.3s;
  }
  
  .filter-btn:hover, .filter-btn.active {
    background-color: #3273dc;
    color: white;
  }
  
  .search-box {
    padding: 8px;
    border: 1px solid #ddd;
    border-radius: 4px;
    margin-left: auto;
    width: 200px;
  }
</style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--   <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
 -->
<!--       <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MMPB: It's Time for Multi-Modal Personalization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/MMPB-Benchmark">Anonymous (TBD)</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Earth</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> --> 
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MMPB-Benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/stackadd/MMPB" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <font size="5">
      <div class="has-text-centered">
        <p>ðŸ”¥<b>What's New</b></p>
      </div>
    </font>
    <font size="4">
      <table width="75%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
        <tr>
          <td>
            <div style="height: 150px; overflow: auto;">
              <ul> 
              <div style="white-space: nowrap;">
                [2025.05.14] ðŸš€ Huggingface Dataset and evaluation code are available!
              </div>
              </ul>
            </div>
          </td>
        </tr>
      </tbody>
    </table>
  </font>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first Visual-Question Answering (VQA) Benchmark for evaluating Vision-Lanugage assistant Models (VLMs) on Personalization!
          </p>
          <p>
            MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: Human, Animal, Object, and Character, with the human category enriched with preference-grounded queries. 
            We structure personalization into three main task types: Awareness, Appropriateness, and Coherency
            
          </p>
          <p>
             Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. 
          </p>
        </div>
      </div>
    </div>

  </div>
</section>



<section class="section" id="Table">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard"><span class="mathvista" style="vertical-align: middle">Overview of MMPB</span></h2>
        <div class="content">
          Examples of personalized queries across task types and representative failure cases of recent VLMs.<br>
          We highlight the lack of VQA benchmarks for effectively and extensively evaluating VLMs.
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="60%" src="images/mot_fig-1.png">     
            </div>
          </centering>
          <br />(a) Some closed-source models exhibit evasive responses. 
          <br />(b) Most VLMs fails to be personalized.
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section" id="Table">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard"><span class="mathvista" style="vertical-align: middle">Evaluation of widely used VLMs on MMPB</span></h2>
        <div class="content">

          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="60%" src="images/main0515-1.png">     
            </div>
          </centering>
          
        </div>
      </div>
    </div>
  </div>
</section>  


<section class="section" id="interactive-table">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Interactive Model Comparison</h2>

        <div class="filter-controls">
          <button class="filter-btn active" data-filter="all">All Models</button>
          <button class="filter-btn" data-filter="open">Open Models</button>
          <button class="filter-btn" data-filter="closed">Closed Models</button>
          <input type="text" class="search-box" placeholder="Search models...">
        </div>

        <table class="data-table">
          <thead>
            <tr>
              <th>Model <button class="sort-btn" data-sort="name">â†•</button></th>
              <th>Text Turn 0 <button class="sort-btn" data-sort="text0">â†•</button></th>
              <th>Image Turn 0 <button class="sort-btn" data-sort="img0">â†•</button></th>
              <th>Text Turn 10 <button class="sort-btn" data-sort="text10">â†•</button></th>
              <th>Image Turn 10 <button class="sort-btn" data-sort="img10">â†•</button></th>
              <th>Details</th>
            </tr>
          </thead>
          <tbody>

          <tr data-category="open">
              <td>Ovis2-34B</td>
              <td>76.2</td>
              <td>72.4</td>
              <td>66.1</td>
              <td>62.0</td>
              <td><button class="expand-btn" data-row="1">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-1">
              <td colspan="6">
                  <div>
                      <h3>Ovis2-34B Details</h3>
                      <p>Model Type: Open Multimodal Language Model</p>
                      <p>Parameters: 34B</p>
                      <p>Strengths: Strong vision and text alignment, good performance in multimodal benchmarks.</p>
                      <p>Link: <a href="#" target="_blank">To be confirmed (e.g., Hugging Face, Project Page)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Qwen2-VL-72B</td>
              <td>73.0</td>
              <td>71.9</td>
              <td>68.5</td>
              <td>60.0</td>
              <td><button class="expand-btn" data-row="2">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-2">
              <td colspan="6">
                  <div>
                      <h3>Qwen2-VL-72B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 72B</p>
                      <p>Strengths: Advanced vision-language understanding, supports high-resolution inputs, strong in Chinese and English.</p>
                      <p>Link: <a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct" target="_blank">Qwen2-VL-72B on Hugging Face (example link)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>InternVL2.5-38B-MPO</td>
              <td>72.2</td>
              <td>66.3</td>
              <td>63.7</td>
              <td>46.4</td>
              <td><button class="expand-btn" data-row="3">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-3">
              <td colspan="6">
                  <div>
                      <h3>InternVL2.5-38B-MPO Details</h3>
                      <p>Model Type: Open Multimodal Language Model (MLLM)</p>
                      <p>Parameters: 38B (Vision: InternViT-6B, LLM: Qwen2.5-32B-Instruct)</p>
                      <p>Strengths: Advanced multimodal understanding, optimized with Mixed Preference Optimization (MPO), competitive with proprietary models.</p>
                      <p>Link: <a href="https://github.com/OpenGVLab/InternVL" target="_blank">InternVL Project Page</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Ovis2-16B</td>
              <td>71.5</td>
              <td>71.7</td>
              <td>64.6</td>
              <td>64.9</td>
              <td><button class="expand-btn" data-row="4">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-4">
              <td colspan="6">
                  <div>
                      <h3>Ovis2-16B Details</h3>
                      <p>Model Type: Open Multimodal Language Model</p>
                      <p>Parameters: 16B</p>
                      <p>Strengths: Vision-language alignment, good performance for its size.</p>
                      <p>Link: <a href="#" target="_blank">To be confirmed (e.g., Hugging Face, Project Page)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Qwen2.5-VL-72B</td>
              <td>70.4</td>
              <td>68.1</td>
              <td>63.9</td>
              <td>57.9</td>
              <td><button class="expand-btn" data-row="5">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-5">
              <td colspan="6">
                  <div>
                      <h3>Qwen2.5-VL-72B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 72B</p>
                      <p>Strengths: Latest iteration of Qwen VL models, enhanced multimodal capabilities.</p>
                      <p>Link: <a href="https://huggingface.co/Qwen" target="_blank">Qwen Models on Hugging Face (general)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="closed">
              <td>Claude-3.5-Sonnet</td>
              <td>68.8</td>
              <td>40.4</td>
              <td>54.3</td>
              <td>41.6</td>
              <td><button class="expand-btn" data-row="6">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-6">
              <td colspan="6">
                  <div>
                      <h3>Claude-3.5-Sonnet Details</h3>
                      <p>Model Type: Closed Multimodal Model</p>
                      <p>Parameters: Unknown (Proprietary)</p>
                      <p>Strengths: Balances speed and intelligence, strong in coding, visual understanding (charts, text from images), and nuanced writing. 200K context window.</p>
                      <p>Link: <a href="https://www.anthropic.com/news/claude-3-5-sonnet" target="_blank">Anthropic Claude 3.5 Sonnet</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>DeepSeek-VL-V2</td>
              <td>68.2</td>
              <td>56.0</td>
              <td>58.5</td>
              <td>60.9</td>
              <td><button class="expand-btn" data-row="7">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-7">
              <td colspan="6">
                  <div>
                      <h3>DeepSeek-VL-V2 Details</h3>
                      <p>Model Type: Open Vision-Language Model (Mixture-of-Experts)</p>
                      <p>Parameters: Varies (e.g., 4.5B activated parameters for the base model, other variants exist)</p>
                      <p>Strengths: Visual question answering, OCR, document/table/chart understanding, visual grounding.</p>
                      <p>Link: <a href="https://github.com/deepseek-ai/DeepSeek-VL2" target="_blank">DeepSeek-VL2 on GitHub</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>LLaVA-OV-72B</td>
              <td>67.4</td>
              <td>58.7</td>
              <td>61.4</td>
              <td>56.5</td>
              <td><button class="expand-btn" data-row="8">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-8">
              <td colspan="6">
                  <div>
                      <h3>LLaVA-OV-72B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 72B</p>
                      <p>Strengths: Open-Vocabulary visual understanding, instruction following with large parameter count.</p>
                      <p>Link: <a href="https://llava-vl.github.io/" target="_blank">LLaVA Project Page (general)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Qwen2-VL-7B</td>
              <td>66.6</td>
              <td>60.6</td>
              <td>62.9</td>
              <td>59.1</td>
              <td><button class="expand-btn" data-row="9">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-9">
              <td colspan="6">
                  <div>
                      <h3>Qwen2-VL-7B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 7B</p>
                      <p>Strengths: Efficient vision-language understanding, good balance of performance and resource requirements.</p>
                      <p>Link: <a href="https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct" target="_blank">Qwen2-VL-7B on Hugging Face (example link)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="closed">
              <td>Gemini-2.0-Flash</td>
              <td>66.5</td>
              <td>66.4</td>
              <td>58.4</td>
              <td>52.2</td>
              <td><button class="expand-btn" data-row="10">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-10">
              <td colspan="6">
                  <div>
                      <h3>Gemini-2.0-Flash Details</h3>
                      <p>Model Type: Closed Multimodal Model</p>
                      <p>Parameters: Unknown (Proprietary)</p>
                      <p>Strengths: Superior speed, built-in tool use, multimodal generation (text, audio, images, video), 1M token context window.</p>
                      <p>Link: <a href="https://deepmind.google/technologies/gemini/" target="_blank">Google Gemini</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="closed">
              <td>Gemini-1.5-Flash</td>
              <td>66.4</td>
              <td>64.1</td>
              <td>61.4</td>
              <td>56.2</td>
              <td><button class="expand-btn" data-row="11">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-11">
              <td colspan="6">
                  <div>
                      <h3>Gemini-1.5-Flash Details</h3>
                      <p>Model Type: Closed Multimodal Model</p>
                      <p>Parameters: Unknown (Proprietary)</p>
                      <p>Strengths: Fast and efficient, multimodal capabilities, long-context understanding.</p>
                      <p>Link: <a href="https://deepmind.google/technologies/gemini/flash/" target="_blank">Google Gemini 1.5 Flash</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="closed">
              <td>GPT-4o</td>
              <td>66.1</td>
              <td>49.1</td>
              <td>64.7</td>
              <td>50.0</td>
              <td><button class="expand-btn" data-row="12">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-12">
              <td colspan="6">
                  <div>
                      <h3>GPT-4o Details</h3>
                      <p>Model Type: Closed Multimodal Model ("omni")</p>
                      <p>Parameters: Unknown (Proprietary)</p>
                      <p>Strengths: Handles text, audio, and image input/output; faster and cheaper than GPT-4 Turbo; strong multilingual and vision capabilities. 128K context window.</p>
                      <p>Link: <a href="https://openai.com/index/hello-gpt-4o/" target="_blank">OpenAI GPT-4o</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>InternVL2.5-26B-MPO</td>
              <td>65.0</td>
              <td>58.3</td>
              <td>58.1</td>
              <td>53.3</td>
              <td><button class="expand-btn" data-row="13">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-13">
              <td colspan="6">
                  <div>
                      <h3>InternVL2.5-26B-MPO Details</h3>
                      <p>Model Type: Open Multimodal Language Model (MLLM)</p>
                      <p>Parameters: 26B (Vision: InternViT-6B, LLM: internlm2_5-20b-chat)</p>
                      <p>Strengths: Efficient multimodal reasoning, mathematics, document understanding, optimized with MPO.</p>
                      <p>Link: <a href="https://github.com/OpenGVLab/InternVL" target="_blank">InternVL Project Page</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Ovis2-8B</td>
              <td>64.5</td>
              <td>62.6</td>
              <td>60.2</td>
              <td>58.4</td>
              <td><button class="expand-btn" data-row="14">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-14">
              <td colspan="6">
                  <div>
                      <h3>Ovis2-8B Details</h3>
                      <p>Model Type: Open Multimodal Language Model</p>
                      <p>Parameters: 8B</p>
                      <p>Strengths: Strong at vision + text alignment, efficient for its size.</p>
                      <p>Link: <a href="#" target="_blank">To be confirmed (e.g., Hugging Face, Project Page)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Qwen2.5-VL-7B</td>
              <td>62.7</td>
              <td>59.1</td>
              <td>57.6</td>
              <td>55.0</td>
              <td><button class="expand-btn" data-row="15">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-15">
              <td colspan="6">
                  <div>
                      <h3>Qwen2.5-VL-7B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 7B</p>
                      <p>Strengths: Efficient and capable smaller VL model from the Qwen2.5 series.</p>
                      <p>Link: <a href="https://huggingface.co/Qwen" target="_blank">Qwen Models on Hugging Face (general)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>InternVL2.5-8B-MPO</td>
              <td>60.6</td>
              <td>61.4</td>
              <td>56.3</td>
              <td>55.2</td>
              <td><button class="expand-btn" data-row="16">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-16">
              <td colspan="6">
                  <div>
                      <h3>InternVL2.5-8B-MPO Details</h3>
                      <p>Model Type: Open Multimodal Language Model (MLLM)</p>
                      <p>Parameters: 8B (Vision: InternViT-300M, LLM: internlm2_5-7b-chat)</p>
                      <p>Strengths: Enhanced multimodal reasoning abilities through MPO, good performance on benchmarks like MathVista.</p>
                      <p>Link: <a href="https://github.com/OpenGVLab/InternVL" target="_blank">InternVL Project Page</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>Llama-3.2-11B</td>
              <td>60.2</td>
              <td>57.2</td>
              <td>56.9</td>
              <td>56.7</td>
              <td><button class="expand-btn" data-row="17">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-17">
              <td colspan="6">
                  <div>
                      <h3>Llama-3.2-11B Details</h3>
                      <p>Model Type: Open Multimodal Language Model (assumed, based on table context)</p>
                      <p>Parameters: 11B</p>
                      <p>Strengths: Strong text generation (Llama family trait), vision-language capabilities (assumed).</p>
                      <p>Link: <a href="https://ai.meta.com/llama/" target="_blank">Meta Llama (general)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>InternVL2.5-7B-MPO</td>
              <td>60.0</td>
              <td>51.6</td>
              <td>47.2</td>
              <td>40.9</td>
              <td><button class="expand-btn" data-row="18">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-18">
              <td colspan="6">
                  <div>
                      <h3>InternVL2.5-7B-MPO Details</h3>
                      <p>Model Type: Open Multimodal Language Model (MLLM)</p>
                      <p>Parameters: ~7B (Specific composition might vary, e.g. InternLM2.5 7B as LLM backbone)</p>
                      <p>Strengths: Part of the InternVL2.5-MPO series, offering efficient multimodal performance for its size.</p>
                      <p>Link: <a href="https://github.com/OpenGVLab/InternVL" target="_blank">InternVL Project Page</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>LLaVA-NeXT-34B</td>
              <td>57.8</td>
              <td>59.4</td>
              <td>52.4</td>
              <td>52.9</td>
              <td><button class="expand-btn" data-row="19">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-19">
              <td colspan="6">
                  <div>
                      <h3>LLaVA-NeXT-34B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 34B</p>
                      <p>Strengths: Next-generation LLaVA model, improved visual reasoning and instruction following.</p>
                      <p>Link: <a href="https://llavanext.github.io/" target="_blank">LLaVA-NeXT Project Page (if available)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>LLaVA-NeXT-32B</td>
              <td>57.5</td>
              <td>54.9</td>
              <td>58.6</td>
              <td>55.4</td>
              <td><button class="expand-btn" data-row="20">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-20">
              <td colspan="6">
                  <div>
                      <h3>LLaVA-NeXT-32B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 32B</p>
                      <p>Strengths: Similar to LLaVA-NeXT-34B, offering advanced vision-language capabilities.</p>
                      <p>Link: <a href="https://llavanext.github.io/" target="_blank">LLaVA-NeXT Project Page (if available)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>LLaVA-OV-7B</td>
              <td>56.8</td>
              <td>49.8</td>
              <td>52.7</td>
              <td>49.1</td>
              <td><button class="expand-btn" data-row="21">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-21">
              <td colspan="6">
                  <div>
                      <h3>LLaVA-OV-7B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 7B</p>
                      <p>Strengths: Efficient open-vocabulary visual understanding and instruction following.</p>
                      <p>Link: <a href="https://llava-vl.github.io/" target="_blank">LLaVA Project Page (general)</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="open">
              <td>LLaVA-1.5-13B</td>
              <td>53.0</td>
              <td>54.5</td>
              <td>50.3</td>
              <td>50.4</td>
              <td><button class="expand-btn" data-row="22">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-22">
              <td colspan="6">
                  <div>
                      <h3>LLaVA-1.5-13B Details</h3>
                      <p>Model Type: Open Vision-Language Model</p>
                      <p>Parameters: 13B</p>
                      <p>Strengths: Improved version of LLaVA, good general visual instruction following.</p>
                      <p>Link: <a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md" target="_blank">LLaVA Model Zoo</a></p>
                  </div>
              </td>
          </tr>
  
          <tr data-category="closed">
              <td>Claude-3.7-Sonnet</td>
              <td>37.0</td>
              <td>15.8</td>
              <td>33.6</td>
              <td>14.6</td>
              <td><button class="expand-btn" data-row="23">View Details</button></td>
          </tr>
          <tr class="row-details" id="details-23">
              <td colspan="6">
                  <div>
                      <h3>Claude-3.7-Sonnet Details</h3>
                      <p>Model Type: Closed Hybrid Reasoning Multimodal Model</p>
                      <p>Parameters: Unknown (Proprietary)</p>
                      <p>Strengths: First hybrid reasoning model from Anthropic; can do quick responses or extended step-by-step thinking; strong improvements in coding; 128K token output capacity.</p>
                      <p>Link: <a href="https://www.anthropic.com/news/claude-3-7-sonnet" target="_blank">Anthropic Claude 3.7 Sonnet</a></p>
                  </div>
              </td>
          </tr>


          </tbody>
        </table>
      </div>
    </div>
  </div>
</section>

<script>

  document.querySelectorAll('.expand-btn').forEach(button => {
    button.addEventListener('click', () => {
      const rowId = button.getAttribute('data-row');
      const detailsRow = document.getElementById(`details-${rowId}`);
      
      if (detailsRow.classList.contains('visible')) {
        detailsRow.classList.remove('visible');
        button.textContent = 'View Details';
      } else {

        document.querySelectorAll('.row-details.visible').forEach(row => {
          row.classList.remove('visible');
          const rowButton = document.querySelector(`[data-row="${row.id.split('-')[1]}"]`);
          if (rowButton) rowButton.textContent = 'View Details';
        });
        
        detailsRow.classList.add('visible');
        button.textContent = 'Hide Details';
      }
    });
  });

  document.querySelectorAll('.sort-btn').forEach(button => {
    button.addEventListener('click', () => {
      const sortType = button.getAttribute('data-sort');
      const tbody = document.querySelector('.data-table tbody');
      const rows = Array.from(tbody.querySelectorAll('tr:not(.row-details)'));
      
      rows.sort((a, b) => {
        let aValue, bValue;
        
        if (sortType === 'name') {
          aValue = a.cells[0].textContent;
          bValue = b.cells[0].textContent;
          return aValue.localeCompare(bValue);
        } else {

          let columnIndex;
          switch (sortType) {
            case 'text0': columnIndex = 1; break;
            case 'img0': columnIndex = 2; break;
            case 'text10': columnIndex = 3; break;
            case 'img10': columnIndex = 4; break;
            default: columnIndex = 0;
          }
          
          aValue = parseFloat(a.cells[columnIndex].textContent);
          bValue = parseFloat(b.cells[columnIndex].textContent);
          return bValue - aValue; 
        }
      });

      const sortedRows = document.createDocumentFragment();
      rows.forEach(row => {
        sortedRows.appendChild(row);
        const rowId = row.querySelector('.expand-btn').getAttribute('data-row');
        const detailsRow = document.getElementById(`details-${rowId}`);
        if (detailsRow) {
          sortedRows.appendChild(detailsRow);
        }
      });
      
      tbody.innerHTML = '';
      tbody.appendChild(sortedRows);
    });
  });
  
  document.querySelectorAll('.filter-btn').forEach(button => {
    button.addEventListener('click', () => {
      document.querySelectorAll('.filter-btn').forEach(btn => {
        btn.classList.remove('active');
      });
      button.classList.add('active');
      
      const filter = button.getAttribute('data-filter');
      const rows = document.querySelectorAll('.data-table tbody tr:not(.row-details)');
      
      rows.forEach(row => {
        const category = row.getAttribute('data-category');
        const detailsId = row.querySelector('.expand-btn').getAttribute('data-row');
        const detailsRow = document.getElementById(`details-${detailsId}`);
        
        if (filter === 'all' || (category && category.includes(filter))) {
          row.style.display = '';
          if (detailsRow.classList.contains('visible')) {
            detailsRow.style.display = '';
          }
        } else {
          row.style.display = 'none';
          detailsRow.style.display = 'none';
        }
      });
    });
  });
  

  const searchBox = document.querySelector('.search-box');
  searchBox.addEventListener('input', () => {
    const searchTerm = searchBox.value.toLowerCase();
    const rows = document.querySelectorAll('.data-table tbody tr:not(.row-details)');
    
    rows.forEach(row => {
      const modelName = row.cells[0].textContent.toLowerCase();
      const detailsId = row.querySelector('.expand-btn').getAttribute('data-row');
      const detailsRow = document.getElementById(`details-${detailsId}`);
      
      if (modelName.includes(searchTerm)) {
        row.style.display = '';
        if (detailsRow.classList.contains('visible')) {
          detailsRow.style.display = '';
        }
      } else {
        row.style.display = 'none';
        detailsRow.style.display = 'none';
      }
    });
  });
</script>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
<!--       <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/MMPB-Benchmark" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://mathvista.github.io/">MathVista</a> and <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
  
</body>
</html>
